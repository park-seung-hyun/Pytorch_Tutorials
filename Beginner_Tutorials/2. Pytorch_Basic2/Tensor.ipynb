{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서의 Tensor는 Torch에서와 거의 동일하게 동작합니다.\n",
    "\n",
    "초기화되지 않은 (5 x 7) 크기의 tensor를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 1.5846e+29, 0.0000e+00, 1.5846e+29, 1.8696e-28, 1.4013e-45,\n",
      "         1.8769e-28],\n",
      "        [1.4013e-45, 1.8769e-28, 1.4013e-45, 1.5414e-44, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.1210e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.5846e+29, 0.0000e+00, 1.5846e+29, 2.0739e-43, 0.0000e+00,\n",
      "         0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.empty(5, 7, dtype=torch.float)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 0, 분산 1의 정규분포를 따르는 무작위 숫자로 dobule tensor를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3898, -3.0912, -0.9900,  0.5391, -0.3190, -0.9116,  0.0197],\n",
      "        [-1.7803, -0.2128, -0.7140, -0.5738, -0.6588, -0.1586, -0.3679],\n",
      "        [-1.7442,  1.6860,  0.9742,  0.8534, -0.8846,  1.0212,  0.6710],\n",
      "        [-2.1215,  1.5603, -0.7039,  0.2931,  0.4925, -1.0324,  0.0213],\n",
      "        [ 1.2850,  0.5222, -2.6728,  1.3624, -0.0112, -0.1919, -0.9335]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size 는 튜플(tuple)과 같으며, 모든 튜플 연산에 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN-PLACE / OUT-OF-PLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 차이점은 tensor의 모든 In-place 연산은 _ 접미사를 갖는다는 것입니다. <br/>예를 들어, add 는 연산 결과를 돌려주는 Out-of-place 연산을 하고, add__ 는 In-place 연산을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
      "       dtype=torch.float64) tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]],\n",
      "       dtype=torch.float64) tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a가 값 3.5로 채워집니다.\n",
    "# fill로 하면 오류\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a는 여전히 3.5입니다.\n",
    "# add_를 사용하면 같이 변합니다.\n",
    "# 3.5 + 4.0 = 7.5의 값이 반환되어 새로운 tensor b가 됩니다.\n",
    "\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "narrow 와 같은 일부 연산들은 In-place 형태를 갖지 않기 때문에 .narrow_ 는 존재하지 않습니다. <br\\>\n",
    "또한, fill_ 은 Out-of-place 형태를 갖지 않기 떄문에 역시 .fill 도 존재하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-인덱스(ZERO INDEXING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 차이점은 Tensor의 인덱스는 0부터 시작(0-인덱스)는 점입니다. (Lua에서 tensor는 1-인덱스를 갖습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
      "       dtype=torch.float64)\n",
      "tensor(3.5000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "b = a[0, 3]  # select 1st row, 4th column from a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python의 슬라이싱(slicing)으로도 Tensor를 인덱스 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카멜표기법(CAMEL CASE) 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 카멜표기법을 사용하지 않는 사소한 차이가 있습니다. 예를 들어 indexAdd 는 index_add_ 라고 표기합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(5, 2)\n",
    "z[:, 0] = 10\n",
    "# 행은 전체, 열은 0번째\n",
    "z[:, 1] = 100\n",
    "# 행은 전체, 열은 1번째\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[601.,  11., 111.,  11.,  41.],\n",
      "        [601.,  11., 111.,  11.,  41.],\n",
      "        [601.,  11., 111.,  11.,  41.],\n",
      "        [601.,  11., 111.,  11.,  41.],\n",
      "        [601.,  11., 111.,  11.,  41.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.tensor([4, 0], dtype=torch.long), z)\n",
    "# index_add(dim, index, tensor)\n",
    "# [4, 0] -> 4번째 열에는 10, 0번째 열에는 100을 더합니다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY 변환(BRIDGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Tensor를 NumPy 배열(array)로 변환하거나, 그 반대로 하는 것은 매우 쉽습니다. <br\\>\n",
    "Torch Tensor와 NumPy 배열은 저장 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Tensor를 NumPy 배열로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "# 위의 a torch tensor를 numpy로 변환\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "# a, b는 각각 torch, numpy지만 저장 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다.\n",
    "print(a)\n",
    "print(b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy 배열을 Torch Tensor로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "# 위의 a numpy를 torch tensor로 변환\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CharTensor를 제외한 CPU 상의 모든 Tensor는 NumPy로의 변환을 지원하며, (NumPy에서 Tensor로의) 반대 변환도 지원합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서 CUDA Tensor는 멋지고 쉽습니다. 그리고 CUDA Tensor를 CPU에서 GPU로 옮겨도 기본 형식(underlying type)은 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이 코드는 CUDA가 사용 가능한 환경에서만 실행됩니다.\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # LongTensor를 생성하고 이를 torch.cuda.LongTensor로 GPU로 옮깁니다.\n",
    "    a = torch.full((10,), 3, device=torch.device(\"cuda\"))\n",
    "    print(type(a))\n",
    "    b = a.to(torch.device(\"cpu\"))\n",
    "    # CPU로 다시 전송을 하면, torch.LongTensor로 되돌아옵니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
