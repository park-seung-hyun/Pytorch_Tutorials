{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn 패키지는 autograd를 완벽하게 통합하기 위해 재설계(redesign)하였습니다. 무엇이 변경되었는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd로 컨테이너(container)를 교체:\n",
    "\n",
    "    ConcatTable 같은 컨테이너나 CAddTable 같은 모듈, 또는 nngraph를 사용하거나 디버깅하기 위한 컨테이너들은 이제 더 이상 사용하지 않습니다. 대신 더 깔끔한 autograd를 사용해서 신경망을 정의할 것입니다. 예를 들면,\n",
    "\n",
    "    - output = nn.CAddTable():forward({input1, input2}) 대신, output = input1 + input2 를 사용합니다.\n",
    "    - output = nn.MulConstant(0.5):forward(input) 대신, output = input * 0.5 를 사용합니다.\n",
    "    \n",
    "상태(state)는 모듈 내에 저장되지 않고, 신경망 그래프 상에 존재합니다:\n",
    "\n",
    "    덕분에 순환신경망을 사용하는 방법이 더 간단해졌습니다. 이제 순환신경망을 만들 때, 더 이상 가중치(weight) 공유에 대해서는 생각할 필요 없이 동일한 Linear 계층(layer)을 여러 차례 호출하면 됩니다.\n",
    "    \n",
    "   ![](./image/NN.png)\n",
    "    \n",
    "    \n",
    "간소화된 디버깅(debugging):\n",
    "\n",
    "    디버깅은 Python의 pdb 디버거를 사용하여 직관적이며, 디버거와 스택 추적(stack trace)은 에러가 발생한 곳에서 정확히 멈춥니다. 이제 보이는대로 얻을 것입니다. (What you see is what you get.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제1: 합성곱 신경망(CONVNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 어떻게 작은 합성곱 신경망을 만드는지 살펴보겠습니다.\n",
    "\n",
    "모든 신경망은 기본(base) 클래스인 nn.Module 로부터 파생됩니다:\n",
    "\n",
    "- 생성자(constructor)에서, 사용할 모든 계층(layer)을 선언합니다.\n",
    "- 순전파(forward) 함수에서, 신경망 모델이 입력에서 출력까지 어떻게 실행되는지를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "\n",
    "    # 생성자\n",
    "    def __init__(self):\n",
    "        # 여기에서모든 모듈을 초기화해놓고,\n",
    "        # 나중에 여기에 선언한 이름으로 접근할 수 있습니다.\n",
    "        # 사용될 모든 계층(layer)을 선언합니다.\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # 순전파 함수에서 신경망의 구조를 정의합니다.\n",
    "    # 여기에서는 단 하나의 입력만 받지만, 필요하면 더 받도록 변경하면 됩니다.\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # 모델 구조를 정의할 때는 어떤 Python 코드를 사용해도 괜찮습니다.\n",
    "        # 모든 코드는 autograd에 의해 올바르고 완벽하게 처리될 것입니다.\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        #\n",
    "        # 심지어 동일한 모듈을 재사용하거나 반복(loop)해도 됩니다.\n",
    "        # 모듈은 더 이상 일시적인 상태를 갖고 있지 않으므로,\n",
    "        # 순전파 과정에서 여러번 사용해도 됩니다.\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 정의한 합성곱 신경망을 사용해봅시다. 먼저 클래스의 인스턴스(instance)를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MNISTConvNet() # 인스턴스 생성\n",
    "print(net) # 신경망 인스턴스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 은 미니 배치(mini-batch)만 지원합니다. torch.nn 패키지 전체는 하나의 샘플이 아닌, 샘플들의 미니배치만을 입력으로 받습니다.\n",
    "\n",
    "예를 들어, nnConv2D 는 nSamples x nChannels x Height x Width 의 4차원 Tensor를 입력으로 합니다.\n",
    "\n",
    "만약 하나의 샘플만 있다면, input.unsqueeze(0) 을 사용해서 가짜 차원을 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무작위 값을 갖는 하나의 미니 배치를 만들어서 합성곱 신경망에 보내보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "# 1개의 sample\n",
    "# 1개의 channel\n",
    "# width = 28\n",
    "# height = 28\n",
    "out = net(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가짜(dummy)로 정답(target)을 하나 만들고, 손실 함수를 사용하여 오차(error)를 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3743, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([3], dtype=torch.long) # 가짜 정답\n",
    "loss_fn = nn.CrossEntropyLoss()  # LogSoftmax + ClassNLL Loss\n",
    "err = loss_fn(out, target) # out vs target\n",
    "err.backward() # d(loss)/dw\n",
    "\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 신경망의 출력 out 은 Tensor 이며, 이를 사용하여 오차를 계산하고 결과를 Tensor 인 err 에 저장합니다. err 에 대해서 .backward 를 호출하면 변화도가 전체 합성곱 신경망의 가중치에 전파됩니다.\n",
    "\n",
    "이제 개별 계층의 가중치(weight)와 변화도(gradient)에 접근해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad.size())\n",
    "# Filter -> 5x5 사용했으므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n",
      "tensor(0.1510)\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.data.norm())  # norm of the weight\n",
    "print(net.conv1.weight.grad.data.norm())  # norm of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순방향/역방향 함수 훅(HOOK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 가중치와 변화도에 대해서 살펴봤습니다. 그렇다면 계층의 출력이나 grad_output 을 살펴보거나 수정하려면 어떻게 해야 할까요?\n",
    "\n",
    "이런 목적으로 사용할 수 있는 훅(Hook) 을 소개합니다.\n",
    "\n",
    "Module 이나 Tensor 에 함수를 등록할 수 있습니다. 훅(Hook)은 순방향 훅과 역방향 훅이 있는데, 순방향 훅은 순전파가 일어날 때 / 역방향 훅은 역전파가 일어날 때 실행됩니다.<br\\> 예제를 살펴보겠습니다.\n",
    "\n",
    "conv2에 전방향 훅을 등록하고 몇 가지 정보를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n",
      "input size: torch.Size([1, 10, 12, 12])\n",
      "output size: torch.Size([1, 20, 8, 8])\n",
      "output norm: tensor(14.3234)\n"
     ]
    }
   ],
   "source": [
    "def printnorm(self, input, output):\n",
    "    # input is a tuple of packed inputs\n",
    "    # output is a Tensor. output.data is the Tensor we are interested\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('')\n",
    "    print('input: ', type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "    print('output norm:', output.data.norm())\n",
    "\n",
    "\n",
    "net.conv2.register_forward_hook(printnorm) # 계층에 훅 등록\n",
    "\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv2에 역방향 훅을 등록하고 몇 가지 정보를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "Inside Conv2d backward\n",
      "\n",
      "input: Inside class:Conv2d \n",
      "<class 'tuple'>\n",
      "\n",
      "input[0]: grad_input:   <class 'torch.Tensor'><class 'tuple'>\n",
      "\n",
      "output: grad_input[0]:   <class 'torch.Tensor'><class 'torch.Tensor'>\n",
      "\n",
      "grad_output: \n",
      " input size:<class 'tuple'> \n",
      "torch.Size([1, 10, 12, 12])grad_output[0]: \n",
      " output size:<class 'torch.Tensor'> \n",
      "torch.Size([1, 20, 8, 8])\n",
      "\n",
      "output norm:grad_input size:  tensor(14.3234)torch.Size([1, 10, 12, 12])\n",
      "\n",
      "grad_output size: torch.Size([1, 20, 8, 8])\n",
      "grad_input norm: tensor(0.0279)\n"
     ]
    }
   ],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].norm())\n",
    "\n",
    "\n",
    "net.conv2.register_backward_hook(printgradnorm)\n",
    "\n",
    "out = net(input)\n",
    "err = loss_fn(out, target)\n",
    "err.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동작하는 전체 MNIST 예제는 여기에서 확인할 수 있습니다. https://github.com/pytorch/examples/tree/master/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제2: 순환 신경망(RECURRENT NETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 PyTorch를 사용하여 순환 신경망을 만들어보겠습니다.\n",
    "\n",
    "신경망의 상태는 계층이 아닌 그래프에 저장되므로, 할 일은 nn.Linear을 생성한 후 순환할 때마다 계속 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "\n",
    "\n",
    "rnn = RNN(50, 20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM과 Penn Tree-bank를 사용한 좀 더 완벽한 언어 모델링(Language Modeling)에 대한 예제는 여기 에 있습니다.\n",
    "\n",
    "PyTorch는 합성곱 신경망과 순환 신경망에 CuDNN 연동을 기본적으로 지원하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 10\n",
    "TIMESTEPS = 5\n",
    "\n",
    "# Create some fake data\n",
    "batch = torch.randn(batch_size, 50)\n",
    "hidden = torch.zeros(batch_size, 20)\n",
    "target = torch.zeros(batch_size, 10)\n",
    "\n",
    "loss = 0\n",
    "for t in range(TIMESTEPS):\n",
    "    # yes! you can reuse the same network several times,\n",
    "    # sum up the losses, and call backward!\n",
    "    hidden, output = rnn(batch, hidden)\n",
    "    loss += loss_fn(output, target)\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
